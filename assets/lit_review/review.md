## #. Literature Review

The use of Machine Learning methods, specifically Neural Networks, for aiding productivity in software development is a very recet practice. And the use of ChatGPT for these purposes is even more recent, with its launch in November 2022 ([OpenAI, 2022](https://openai.com/blog/chatgpt)). As is expected for very recent phenomenons, the literature on the impact of ChatGPT on software development productivity is very scarce. Nonetheless, the undeniable disruptiveness of the application has awakened great interest in its study, which has offset some of the scarcity in the literature. Still, research on the potential, capabilities, and impact of ChatGPT on software development is still in its infancy and the constantly evolving nature of the topic requires quick adaptations of research to adequately update the understanding on present and future questions. The relevance of this last point is illustrated by the fact that the vast majority of the literature on this topic pertains to the GPT-3.5 model, even though GPT-4 was launched in march 2023.

### #.1 Potential of ChatGPT on Software Development

Abu Jaber, Beganovic and Abd Almisreb (2023) conduct a literature review on the potential of ChatGPT in software development. They outline potential uses for troubleshooting and bug repair, as well as educational applications for these purposes. This includes optimizing programs and numerical algorithms, as well as the potential incorporation of ChatGPT as an integrated controller in Generalized Intelligence. The authors discuss the creation of software solutions and architectures, both comparatively (evaluating multiple responses) and through a dialogical process with the application for software architectural design. They also delve into considerations for Prompt Engineering when employing the program for software development.

Rahmaniar (2023) comes to similar conclusions. The author surveys the potential for productivity gains in the field of software development through assistance in programming, documentation creation, training and onboarding, code review, and interactions with clients and stakeholders. All this thanks to the model's ability to process natural language, generative capacity, flexibility and potential for model learning, dialogical interactivity, the large spectrum of applicability for its use, and contributions to the Open Source space. The author also discusses the negative ramifications of such uses, such as possible incompleteness of generated code, which may cause difficulties for users and potential security vulnerabilities. In this regard, there is also the potential for malicious use of the model to create malware. The author calls for taking ethical considerations into account; for example, during the learning process, models may be exposed to content that integrates and perpetuates biases with negative consequences. With future improvements in language models, especially with the potential of GPT models, response generation would have better performance, translating into greater ease and productivity gains for developers using LLM tools. Future versions could be used for streamlining in the development process, where generation is constant with the aim of optimizing and repairing code in real time.

### #.2 Tests on ChatGPT's capabilities:

One of the potentials of ChatGPT in software development lies in its usefulness in education.  Jalil et al. (2023) test ChatGPT with solved problems from Ammann & Offutt (2016), a widely used introductory texbook on software testing. They find that the model is able to respond correctly to about 43% of software testing questions, and provides a sufficient explanation for its response in the majority of cases.

Another possible avenue for innovation and productivity gains is the use of ChatGPT in the optimization of algorithms and numerical methods. Silvia et al. (2023) use ChatGPT for troubleshooting and optimization of GCode programs for additive manufacturing (3D printing) routines. After 6 iterations of dialogue with the chatbot with the purpose of training on the low-level GCode language, they prompt ChatGPT to solve manufacturing problems resulting from existing routines, and find noticeable improvements in the build quality of the end products after using ChatGPT's output.

As for numerical methods, Kashefi and Mukerji (2023) propose and test the usage of ChatGPT on the generation of code for several numerical methods related to physics, in several programming languages. This involves programming the simulation of simple linear systems as well as more complicated dynamic systems that involves one or more differencial equations. ChatGPT was able to generate sufficient, programs for the mathematical system described by the researchers. Some issues arise like the generation of singular matrices, attempts at opperating on incompatible arrays, and interruptions of the generation of code when the program is lengthy.

It is also possible to use ChatGPT as a controller for routing tasks and sub-tasks to algorithms more suitable for solving them. This is proposed and tested by Shen et al. (2023), who use GPT-4 in conjunction with several specialized models from the HuggingFace platform to solve complex tasks that involve the solution of several specialized sub-tasks. They develop a 4 stage architecture called HuggingGPT: first, ChatGPT generates a plan for solving the complex task through smaller sub-tasks; on the second stage, it chooses the most capable HuggingFace model to solve each sub-task; in stage 3, the sub-tasks are routed to their corresponding models and these, in turn, generate the corresponding output; the fourth and final stage involves parsing these outputs and presenting them to the user. They test this procedure on image captioning, image classification, and object detection. They find that GPT-4 is highly accurate at creating the necessary task plans and classifying sub-tasks to the corresponding specialized models. They also successfuly test HuggingGPT on a variety of complex tasks involving text, images, audio and video.

Ahmad et al. (2023) describe and test human-bot collaboration for the purpose of formally outlining sofware architecture. Their testing starts with outlining an architecture story that describes the conditions that must be satisfied by the software. This outline is fed to the chatbot as an initial prompt as a preliminary step for the testing. After this, a novice software architect enters a dialectical process with the chatbot to analyze, synthesize, and evaluate a potential architectural solution. They conclude that this dialectica, collaborative process can be easily carried out to outline a software architecture more efficiently in the early stages of product development.

Finally, Sobania et al. (2023) and Zhang et al. (2023) test the bug repair capabilities of ChatGPT using the QuickBugs database and a custom database, respectively. Both articles find overall success using the model when prompting it to fix the bugs in the code presented, and that more rounds of dialogue increase the success rate. Sobania et al. (2023) specifically find that ChatGPT is comparable to other LLMs specialized in this task, and that LLMs outperform a standard, RN-based program repair application. On the other hand, Zhang et al. (2023) create a new database of coding problems to test the generalization capabilities of the model, as QuickBugs may have been used for training GPT-3.5. They again find that ChatGPT is capable of giving correct fixes in most scenarios.

### #.3 The Impact of ChatGPT on Software Development Productivity

Gallea (2023) compares the Stack Overflow pages for Python and R, both programming languages used in data science. The reason behind this comparison is the fact that Python is a much more popular programming language with a high amount of potential training material for the model, and the author mentions the use of ChatGPT was not efficient when answer R related question; this would imply a lower impact of ChatGPT on the R language. The data comes from Stack Overflow Explorer, specifically information on the number of questions, the resolution status of the questions, and the score for each of them. The results from the Differences in Differences analysis suggest a negative impact on the quantity of questions, a positive impact on the average score (as a measure of quality) of questions, and a negative impact on the proportion of resolved questions (as a measure of complexity) for Python compared to R.

Saguu and Ante (2023) analyze the returns of crypto-assets related to artificial intelligence. For this purpose, they utilize daily frequency data of crypto-asset prices from Coingecko and CoinMarketCap. The treatment group is considered as those assets related to artificial intelligence, and the start of the post-treatment period is the launch of ChatGPT. They also control for total market capitalization and transaction volume for each asset. They employ both the Differences in Differences and Synthetic Differences in Differences methods. They find that the launch of ChatGPT had a positive impact on the return of those assets related to artificial intelligence on both platforms, compared to those unrelated to AI.

Demirci, Hannane and Xinrong (2023) analyze the impact of ChatGPT on the demand for services for freelance workers on a platform dedicated to facilitating such services. Firstly, they identify service groups based on the skills required to carry them out. Next, they assign an artificial intelligence exposure index to each type of service, indicating the feasibility of using AI to perform services in each group. The analysis is further adjusted using the Google Search Volume Index. They apply the Differences in Differences method between two groups: high and low exposure to artificial intelligence. They find that the impact of ChatGPT was negative on the quantity of postings seeking services in groups with high exposure to AI, compared to those with a low exposure index.

Del Rio-Chanona, Laurentsyeva and Wachs (2023) present another example of an analysis of the impact of ChatGPT on question and answer platforms for programming topics. In this case, the authors compare Stack Overflow with Math Exchange, a platform focused on questions and answers about mathematics; the topics on this platform would be less susceptible to the effects of ChatGPT. They also compare Stack Overflow with its Russian counterpart; and with Segmentdefault, its Chinese counterpart. This is because access to ChatGPT is restricted in these two countries, which would decrease the impact it can have on the software developmnet field. By applying Differences in Differences, the authors find a negative impact on the number of posts per week, the number of questions per week, and the number of posts on weekdays on Stack Overflow compared to the other platforms.

Finally, Kreitmeir & Raschky (2023) analyze the impact on productivity in software development using daily user-level data provided by GitHub. They leverage the ban of ChatGPT in Italy, comparing it to France and Austria, where the application is freely accessible. As productivity measures, the authors use the existence of new releases by each user; the sum of pushes, pull requests, comments on pull requests, comments on commits, repo creations, and issues; the sum of pushes and pulls; and the total number of events by each user. They also analyze the quantity of new Tor users, as users in countries where access is restricted may bypass the ban through this means, indicating high demand and recognition of the application's utility. The result obtained through the Differences in Differences method indicates a significant negative impact on the probability that each user-day presents a new software release in Italy compared to other countries. This suggests lower productivity as a result of the prohibition.
